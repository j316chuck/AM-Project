{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation\n",
    "from tensorflow.keras import Model\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import zip_longest\n",
    "from Bio.Seq import translate, IUPAC\n",
    "from torch.nn import functional as F\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    :param X: dataframe or numpy array to be normalized\n",
    "    :return: dataframe or numpy array that is normalized\n",
    "    >>> normalize(np.array([0, 1, 2]))\n",
    "    array([-1.22474487,  0.        ,  1.22474487])\n",
    "    \"\"\"\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def get_wild_type_dna_sequence():\n",
    "    \"\"\"\n",
    "    :return: string of wild type dna sequence from cached location\n",
    "    \"\"\"\n",
    "    return \"AGCAAGGGCGAGGAGCTGTTCAC\" \\\n",
    "           \"CGGGGTGGTGCCCATCCTGGTCG\" \\\n",
    "           \"AGCTGGACGGCGACGTAAACGGC\" \\\n",
    "           \"CACAAGTTCAGCGTGTCCGGCGA\" \\\n",
    "           \"GGGCGAGGGCGATGCCACCTACG\" \\\n",
    "           \"GCAAGCTGACCCTGAAGTTCATC\" \\\n",
    "           \"TGCACCACCGGCAAGCTGCCCGT\" \\\n",
    "           \"GCCCTGGCCCACCCTCGTGACCA\" \\\n",
    "           \"CCCTGTCATACGGCGTGCAGTGC\" \\\n",
    "           \"TTCAGCCGCTACCCCGACCACAT\" \\\n",
    "           \"GAAGCAGCACGACTTCTTCAAGT\" \\\n",
    "           \"CCGCCATGCCCGAAGGCTACGTC\" \\\n",
    "           \"CAGGAGCGCACCATCTTCTTCAA\" \\\n",
    "           \"GGACGACGGCAACTACAAGACCC\" \\\n",
    "           \"GCGCCGAGGTGAAGTTCGAGGGC\" \\\n",
    "           \"GACACACTAGTGAACCGCATCGA\" \\\n",
    "           \"GCTGAAGGGCATCGACTTCAAGG\" \\\n",
    "           \"AGGACGGCAACATCCTGGGGCAC\" \\\n",
    "           \"AAGCTGGAGTACAACTACAACAG\" \\\n",
    "           \"CCACAACGTCTATATCATGGCCG\" \\\n",
    "           \"ACAAGCAGAAGAACGGCATCAAG\" \\\n",
    "           \"GTGAACTTCAAGATCCGCCACAA\" \\\n",
    "           \"CATCGAGGACGGCAGCGTGCAGC\" \\\n",
    "           \"TCGCCGACCACTACCAGCAGAACA\" \\\n",
    "           \"CCCCCATCGGCGACGGCCCCGTGC\" \\\n",
    "           \"TGCTGCCCGACAACCACTACCTGA\" \\\n",
    "           \"GCACCCAGTCCGCCCTGAGCAAAGA\" \\\n",
    "           \"CCCCAACGAGAAGCGCGATCACAT\" \\\n",
    "           \"GGTCCTGCTGGAGTTCGTGACCGC\" \\\n",
    "           \"CGCCGGGATCACTCACGGCATGGA\" \\\n",
    "           \"CGAGCTGTACAAGTGA\"\n",
    "\n",
    "\n",
    "def dna_to_amino_acid(dna_seq):\n",
    "    \"\"\"\n",
    "    :param dna_seq: string of dna sequence\n",
    "    :return: dna sequence in amino acid form\n",
    "    >>> dna_to_amino_acid(\"ACTGGCTAT\")\n",
    "    'TGY'\n",
    "    \"\"\"\n",
    "    return translate(dna_seq)\n",
    "\n",
    "\n",
    "def get_all_amino_acids(gap=False):\n",
    "    \"\"\"\n",
    "    :return: string of all amino acids + stop character\n",
    "    >>> get_all_amino_acids(gap=True)\n",
    "    '*ACDEFGHIKLMNPQRSTVWY'\n",
    "    >>> len(get_all_amino_acids(gap=False))\n",
    "    20\n",
    "    >>> get_all_amino_acids(gap=False)\n",
    "    'ACDEFGHIKLMNPQRSTVWY'\n",
    "    \"\"\"\n",
    "    if gap:\n",
    "        return \"*\" + IUPAC.protein.letters  # length 21\n",
    "    else:\n",
    "        return IUPAC.protein.letters  # length 20\n",
    "\n",
    "\n",
    "def get_wild_type_amino_acid_sequence(gap=False):\n",
    "    \"\"\"\n",
    "    :return: string of wild type amino acid sequence from cached location\n",
    "    \"\"\"\n",
    "    if gap:\n",
    "        return dna_to_amino_acid(get_wild_type_dna_sequence())  # length 238\n",
    "    else:\n",
    "        return dna_to_amino_acid(get_wild_type_dna_sequence())[:-1]  # length 237\n",
    "\n",
    "\n",
    "def count_substring_mismatch(s1, s2):\n",
    "    \"\"\"\n",
    "    :param s1: string one\n",
    "    :param s2: string two\n",
    "    :return: int of the number of mismatches between the two sequences\n",
    "    >>> count_substring_mismatch('1', '2')\n",
    "    1\n",
    "    >>> count_substring_mismatch('ACT', 'ACGA')\n",
    "    2\n",
    "    \"\"\"\n",
    "    return sum([i != j for i, j in zip_longest(s1, s2)])\n",
    "\n",
    "\n",
    "def get_gfp_data(amino_acid=False, gfp_data_path=\"../data/gfp_data.csv\", x_feature=\"nucSequence\", y_feature=\"medianBrightness\", normalize_y=True, test_size=0.2, shuffle=False):\n",
    "    \"\"\"\n",
    "    :param amino_acid:  amino acid format or DNA\n",
    "    :param gfp_data_path: gfp data path\n",
    "    :param x_feature: column to use for x data\n",
    "    :param y_feature: column to use for y data\n",
    "    :param normalize_y: normalize y or not\n",
    "    :param test_size: size of test set\n",
    "    :param shuffle: shuffle data or not\n",
    "    :return: gfp data split across train and test set\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(gfp_data_path, index_col=0)\n",
    "    if amino_acid: \n",
    "        x = df[x_feature].apply(lambda x: dna_to_amino_acid(x)).values\n",
    "    else: \n",
    "        x = df[x_feature].values\n",
    "    y = df[y_feature].values\n",
    "    if normalize_y: \n",
    "        y = normalize(y)\n",
    "    return train_test_split(x, y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def save_data(x_train, x_test, y_train, y_test, data_path):\n",
    "    \"\"\"\n",
    "    save your gfp data in location\n",
    "    :param x_train: training data\n",
    "    :param x_test: testing data\n",
    "    :param y_train: training output\n",
    "    :param y_test: testing output\n",
    "    :param data_path: path to save data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    np.save(data_path + \"x_train.npy\", x_train)\n",
    "    np.save(data_path + \"x_test.npy\", x_test)\n",
    "    np.save(data_path + \"y_train.npy\", y_train)\n",
    "    np.save(data_path + \"y_test.npy\", y_test)\n",
    "\n",
    "\n",
    "def load_data(data_path, start_index=None, end_index=None):\n",
    "    \"\"\"\n",
    "    load your data from location\n",
    "    :param data_path: path of your gfp data\n",
    "    :param start_index: starting index of string\n",
    "    :param end_index: end index of string\n",
    "    :return: train test matrices with expected outputs\n",
    "    \"\"\"\n",
    "    x_train = np.load(data_path + \"x_train.npy\")\n",
    "    x_train = [x[start_index:end_index] for x in x_train]  # select sub portion of the string\n",
    "    x_test = np.load(data_path + \"x_test.npy\")\n",
    "    x_test = [x[start_index:end_index] for x in x_test]  # select sub portion of the string\n",
    "    y_train = np.load(data_path + \"y_train.npy\")\n",
    "    y_test = np.load(data_path + \"y_test.npy\")\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_mismatches_histogram(sequences_lst, base_sequences_lst, save_fig_dir=None, show=False):\n",
    "    \"\"\"\n",
    "    counts the minimum mismatch between the sequences and the base_sequences_lst\n",
    "    :param sequences_lst: list of sampled sequences\n",
    "    :param base_sequences_lst: list, base sequences to compare all other sequences against\n",
    "    :param save_fig_dir: saves the histogram of mismatches\n",
    "    :param show: shows the histogram of mismatches\n",
    "    :return: list that counts the number of mismatches from the wild type\n",
    "    >>> plot_mismatches_histogram([\"ACT\", \"ACG\"], [\"ACG\"], None, False)\n",
    "    [1, 0]\n",
    "    >>> plot_mismatches_histogram([\"ACTG\", \"ACCT\"], [\"ACTG\", \"ACCC\"], None, False)\n",
    "    [0, 1]\n",
    "    >>> try:\n",
    "    ...     plot_mismatches_histogram([\"ACT\", \"ACG\"], [\"ACTG\", \"ACCC\"], None, False) # not same length\n",
    "    ... except:\n",
    "    ...     print(\"assertion error\")\n",
    "    assertion error\n",
    "    \"\"\"\n",
    "\n",
    "    assert(all(type(base_seq) is str for base_seq in base_sequences_lst))\n",
    "    assert(all(type(seq) is str for seq in sequences_lst))\n",
    "    assert(all([len(base_sequences_lst[0]) == len(seq) for seq in sequences_lst]))\n",
    "    assert(all([len(base_sequences_lst[0]) == len(base_seq) for base_seq in base_sequences_lst]))\n",
    "    mismatches = []\n",
    "    for seq in sequences_lst:\n",
    "        mismatches.append(min([count_substring_mismatch(base_sequence, seq) for base_sequence in base_sequences_lst]))\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.title(\"mismatches from wild type\", fontsize=15)\n",
    "    plt.hist(mismatches, bins=15)\n",
    "    plt.xlabel(\"mismatches\", fontsize=12)\n",
    "    plt.ylabel(\"counts\", fontsize=12)\n",
    "    if save_fig_dir:\n",
    "        plt.savefig(save_fig_dir)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return mismatches\n",
    "\n",
    "\n",
    "def one_hot_encode(X, alphabet):\n",
    "    \"\"\"\n",
    "    one hot encode a list of strings\n",
    "    :param X: list of sequences represented by the set of letters in alphabet\n",
    "    :param alphabet:\n",
    "    :return: one hot encoded list of X sequences\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Input: X is a list of sequences represented by the set of letters in alphabet\n",
    "        All sequences must be the same length\n",
    "    Output: one hot encoded list of X sequences\n",
    "    Example: one_hot_encode([\"ACT\", \"ACG\"], \"ACTG\") = [[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "                                              [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]]\n",
    "    \"\"\"\n",
    "    assert(len(X) > 0)\n",
    "    assert(all([len(X[0]) == len(X[i]) for i in range(len(X))]))\n",
    "    alphabet_size = len(alphabet)\n",
    "    alphabet_dict = dict(zip(alphabet, range(alphabet_size)))\n",
    "    one_hot_matrix = np.zeros((len(X), alphabet_size * len(X[0])))\n",
    "    for i, sequence in enumerate(X):\n",
    "        for j, letter in enumerate(sequence):\n",
    "            if letter not in alphabet:\n",
    "                raise KeyError(\"letter not in alphabet\")\n",
    "            index = alphabet_dict[letter]\n",
    "            one_hot_matrix[i, alphabet_size * j + index] = 1.0\n",
    "    return one_hot_matrix\n",
    "\n",
    "\n",
    "def one_hot_decode(X, alphabet):\n",
    "    \"\"\"\n",
    "    one hot decode a matrix\n",
    "    :param X: one hot encoded list of DNA Sequences represented by the alphabet\n",
    "    :param alphabet: all the letters in the vocabulary of X\n",
    "    :return: a one hot decoded matrix in list of strings format\n",
    "    >>> one_hot_decode([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], \\\n",
    "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]], \"ACTG\")\n",
    "    ['ACT', 'ACG']\n",
    "    \"\"\"\n",
    "    assert(len(X) > 0)\n",
    "    assert(all([len(X[0]) == len(X[i]) for i in range(len(X))]))\n",
    "    alphabet_size = len(alphabet)\n",
    "    sequences_lst = []\n",
    "    for i, one_hot_sequence in enumerate(X):\n",
    "        sequence, sequence_len = [], len(one_hot_sequence)\n",
    "        for j in range(0, sequence_len, alphabet_size):\n",
    "            index = np.argmax(one_hot_sequence[j:j+alphabet_size])\n",
    "            sequence.append(alphabet[index])\n",
    "        sequences_lst.append(\"\".join(sequence))\n",
    "    return sequences_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_gfp_data()\n",
    "x_train, x_test, y_train, y_test = x_train[:1000], x_test[:1000], y_train[:1000], y_test[:1000]\n",
    "train_x = one_hot_encode(x_train, \"ACTG\")\n",
    "test_x = one_hot_encode(x_test, \"ACTG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.d1 = Dense(200, name='d1')\n",
    "        self.r1 = Activation('relu', name = 'r1')\n",
    "        self.d2 = Dense(100, name='d2')\n",
    "        self.r2 = Activation('relu', name = 'r1')\n",
    "        self.d3 = Dense(1, name='d3')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.r2(x)\n",
    "        return self.d3(x)\n",
    "        \n",
    "    \n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define own function\n",
    "def train_step(model, inputs, outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_object(outputs, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    \n",
    "def test_step(model, inputs, outputs):\n",
    "    predictions = model(inputs)\n",
    "    t_loss = loss_object(outputs, predictions)\n",
    "    test_loss(t_loss)\n",
    "    \n",
    "def train(model, train_ds, test_ds, EPOCHS=1): \n",
    "    for epoch in range(EPOCHS):\n",
    "        for inputs, outputs in train_ds:\n",
    "            train_step(model, inputs, outputs)\n",
    "\n",
    "        for inputs, outputs in test_ds:\n",
    "            test_step(model, inputs, outputs)\n",
    "        \n",
    "        template = 'Epoch {}, Loss: {}, Test Loss: {}'\n",
    "        print(template.format(epoch+1,\n",
    "                            train_loss.result().numpy(),\n",
    "                            test_loss.result().numpy(),))\n",
    "        \n",
    "            \n",
    "        \n",
    "        # Reset the metrics for the next epoch\n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0124127864837646, Test Loss: 0.9231374859809875%\n",
      "Epoch 2, Loss: 1.0476630926132202, Test Loss: 0.904578685760498%\n",
      "Epoch 3, Loss: 1.023443579673767, Test Loss: 0.8900732398033142%\n",
      "Epoch 4, Loss: 1.0021580457687378, Test Loss: 0.8783804774284363%\n",
      "Epoch 5, Loss: 0.9866369366645813, Test Loss: 0.8687545657157898%\n",
      "Epoch 6, Loss: 0.9681664705276489, Test Loss: 0.8610106110572815%\n",
      "Epoch 7, Loss: 0.962997555732727, Test Loss: 0.8544328212738037%\n",
      "Epoch 8, Loss: 0.9510301947593689, Test Loss: 0.8495440483093262%\n",
      "Epoch 9, Loss: 0.9423377513885498, Test Loss: 0.8457207083702087%\n",
      "Epoch 10, Loss: 0.9243901968002319, Test Loss: 0.8428305387496948%\n",
      "Epoch 11, Loss: 0.9274917244911194, Test Loss: 0.8406656980514526%\n",
      "Epoch 12, Loss: 0.9188932180404663, Test Loss: 0.8392453193664551%\n",
      "Epoch 13, Loss: 0.9118848443031311, Test Loss: 0.83846116065979%\n",
      "Epoch 14, Loss: 0.9094499945640564, Test Loss: 0.8380914926528931%\n",
      "Epoch 15, Loss: 0.9091137647628784, Test Loss: 0.8380975723266602%\n",
      "Epoch 16, Loss: 0.9051934480667114, Test Loss: 0.8383560180664062%\n",
      "Epoch 17, Loss: 0.9005297422409058, Test Loss: 0.8388078212738037%\n",
      "Epoch 18, Loss: 0.8976688385009766, Test Loss: 0.8393698930740356%\n",
      "Epoch 19, Loss: 0.89778733253479, Test Loss: 0.8400325775146484%\n",
      "Epoch 20, Loss: 0.8964683413505554, Test Loss: 0.8407523036003113%\n",
      "Epoch 21, Loss: 0.8939660787582397, Test Loss: 0.8414520621299744%\n",
      "Epoch 22, Loss: 0.8962739706039429, Test Loss: 0.8422054052352905%\n",
      "Epoch 23, Loss: 0.8865265846252441, Test Loss: 0.8428167104721069%\n",
      "Epoch 24, Loss: 0.8944718837738037, Test Loss: 0.8435539603233337%\n",
      "Epoch 25, Loss: 0.8899112343788147, Test Loss: 0.8441686630249023%\n",
      "Epoch 26, Loss: 0.8975000381469727, Test Loss: 0.8447705507278442%\n",
      "Epoch 27, Loss: 0.8854557871818542, Test Loss: 0.845135509967804%\n",
      "Epoch 28, Loss: 0.8880585432052612, Test Loss: 0.8456990122795105%\n",
      "Epoch 29, Loss: 0.8907415866851807, Test Loss: 0.8461406230926514%\n",
      "Epoch 30, Loss: 0.8894504308700562, Test Loss: 0.8465779423713684%\n",
      "Epoch 31, Loss: 0.8940014839172363, Test Loss: 0.846885621547699%\n",
      "Epoch 32, Loss: 0.8884432315826416, Test Loss: 0.8471769094467163%\n",
      "Epoch 33, Loss: 0.8901824951171875, Test Loss: 0.8474138379096985%\n",
      "Epoch 34, Loss: 0.8874918222427368, Test Loss: 0.8476510643959045%\n",
      "Epoch 35, Loss: 0.8915812969207764, Test Loss: 0.8479393720626831%\n",
      "Epoch 36, Loss: 0.8895899057388306, Test Loss: 0.848109781742096%\n",
      "Epoch 37, Loss: 0.8892258405685425, Test Loss: 0.8482437133789062%\n",
      "Epoch 38, Loss: 0.889967679977417, Test Loss: 0.8484187722206116%\n",
      "Epoch 39, Loss: 0.8876420855522156, Test Loss: 0.8484587669372559%\n",
      "Epoch 40, Loss: 0.8937078714370728, Test Loss: 0.8487136363983154%\n",
      "Epoch 41, Loss: 0.8870396614074707, Test Loss: 0.848741888999939%\n",
      "Epoch 42, Loss: 0.8830987215042114, Test Loss: 0.8488336801528931%\n",
      "Epoch 43, Loss: 0.8928707838058472, Test Loss: 0.8489950895309448%\n",
      "Epoch 44, Loss: 0.8877903819084167, Test Loss: 0.849067747592926%\n",
      "Epoch 45, Loss: 0.8875712752342224, Test Loss: 0.8492342233657837%\n",
      "Epoch 46, Loss: 0.8891844749450684, Test Loss: 0.8492305278778076%\n",
      "Epoch 47, Loss: 0.8853636980056763, Test Loss: 0.8492369651794434%\n",
      "Epoch 48, Loss: 0.8840071558952332, Test Loss: 0.8493717908859253%\n",
      "Epoch 49, Loss: 0.883536696434021, Test Loss: 0.8493915796279907%\n",
      "Epoch 50, Loss: 0.886194109916687, Test Loss: 0.849518895149231%\n",
      "Epoch 51, Loss: 0.8850791454315186, Test Loss: 0.8495913147926331%\n",
      "Epoch 52, Loss: 0.88914954662323, Test Loss: 0.8497158885002136%\n",
      "Epoch 53, Loss: 0.8846494555473328, Test Loss: 0.8497937321662903%\n",
      "Epoch 54, Loss: 0.8857635855674744, Test Loss: 0.8497824668884277%\n",
      "Epoch 55, Loss: 0.8872571587562561, Test Loss: 0.8498446345329285%\n",
      "Epoch 56, Loss: 0.8847374320030212, Test Loss: 0.8498333692550659%\n",
      "Epoch 57, Loss: 0.8900948762893677, Test Loss: 0.8499164581298828%\n",
      "Epoch 58, Loss: 0.8846246004104614, Test Loss: 0.8498462438583374%\n",
      "Epoch 59, Loss: 0.8889103531837463, Test Loss: 0.8499536514282227%\n",
      "Epoch 60, Loss: 0.8877055644989014, Test Loss: 0.8498606085777283%\n",
      "Epoch 61, Loss: 0.8879599571228027, Test Loss: 0.8498759269714355%\n",
      "Epoch 62, Loss: 0.8894314169883728, Test Loss: 0.8498510122299194%\n",
      "Epoch 63, Loss: 0.889629065990448, Test Loss: 0.8499156832695007%\n",
      "Epoch 64, Loss: 0.8867371082305908, Test Loss: 0.8498930931091309%\n",
      "Epoch 65, Loss: 0.8921995162963867, Test Loss: 0.8498853445053101%\n",
      "Epoch 66, Loss: 0.8873564600944519, Test Loss: 0.8498493432998657%\n",
      "Epoch 67, Loss: 0.8888310194015503, Test Loss: 0.8498272895812988%\n",
      "Epoch 68, Loss: 0.8913288116455078, Test Loss: 0.8498382568359375%\n",
      "Epoch 69, Loss: 0.8852483630180359, Test Loss: 0.8497889637947083%\n",
      "Epoch 70, Loss: 0.8900903463363647, Test Loss: 0.8498530983924866%\n",
      "Epoch 71, Loss: 0.8902021646499634, Test Loss: 0.849801242351532%\n",
      "Epoch 72, Loss: 0.8864853978157043, Test Loss: 0.8497907519340515%\n",
      "Epoch 73, Loss: 0.8826136589050293, Test Loss: 0.8498165607452393%\n",
      "Epoch 74, Loss: 0.8950273990631104, Test Loss: 0.849970281124115%\n",
      "Epoch 75, Loss: 0.8876268267631531, Test Loss: 0.8498042821884155%\n",
      "Epoch 76, Loss: 0.8900614976882935, Test Loss: 0.8498395681381226%\n",
      "Epoch 77, Loss: 0.888206958770752, Test Loss: 0.8498017191886902%\n",
      "Epoch 78, Loss: 0.8864760398864746, Test Loss: 0.8498611450195312%\n",
      "Epoch 79, Loss: 0.8852514624595642, Test Loss: 0.8498320579528809%\n",
      "Epoch 80, Loss: 0.8873828649520874, Test Loss: 0.8499083518981934%\n",
      "Epoch 81, Loss: 0.8795381188392639, Test Loss: 0.8498634099960327%\n",
      "Epoch 82, Loss: 0.8892946243286133, Test Loss: 0.8499592542648315%\n",
      "Epoch 83, Loss: 0.8926024436950684, Test Loss: 0.850044846534729%\n",
      "Epoch 84, Loss: 0.885991096496582, Test Loss: 0.8499502539634705%\n",
      "Epoch 85, Loss: 0.8897067904472351, Test Loss: 0.8500082492828369%\n",
      "Epoch 86, Loss: 0.88265460729599, Test Loss: 0.8499350547790527%\n",
      "Epoch 87, Loss: 0.8894037008285522, Test Loss: 0.8499999642372131%\n",
      "Epoch 88, Loss: 0.8920639753341675, Test Loss: 0.850014865398407%\n",
      "Epoch 89, Loss: 0.8882371187210083, Test Loss: 0.8498895168304443%\n",
      "Epoch 90, Loss: 0.8896832466125488, Test Loss: 0.849981427192688%\n",
      "Epoch 91, Loss: 0.8886449933052063, Test Loss: 0.8499675989151001%\n",
      "Epoch 92, Loss: 0.8843129873275757, Test Loss: 0.849919855594635%\n",
      "Epoch 93, Loss: 0.8859574794769287, Test Loss: 0.8499436974525452%\n",
      "Epoch 94, Loss: 0.8921207785606384, Test Loss: 0.8500056862831116%\n",
      "Epoch 95, Loss: 0.8843052387237549, Test Loss: 0.8499824404716492%\n",
      "Epoch 96, Loss: 0.8868989944458008, Test Loss: 0.8499716520309448%\n",
      "Epoch 97, Loss: 0.8915642499923706, Test Loss: 0.8499525189399719%\n",
      "Epoch 98, Loss: 0.8900641202926636, Test Loss: 0.8499467968940735%\n",
      "Epoch 99, Loss: 0.8900371193885803, Test Loss: 0.8498764038085938%\n",
      "Epoch 100, Loss: 0.886962354183197, Test Loss: 0.8497328758239746%\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, test_dataset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.convert_to_tensor(train_x[0]), (1, -1))\n",
    "length = len(x_train[0]) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype float32 for Tensor with dtype float64: <tf.Tensor: id=442939, shape=(1, 2856), dtype=float64, numpy=array([[1., 0., 0., ..., 0., 0., 0.]])>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-5a98145bbfef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mam_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    233\u001b[0m                         shape=None):\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2554\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2556\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1404\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[1;32m   1537\u001b[0m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m                 name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m   1539\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1269\u001b[0m       raise ValueError(\n\u001b[1;32m   1270\u001b[0m           \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m           (dtype.name, value.dtype.name, value))\n\u001b[0m\u001b[1;32m   1272\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype float64: <tf.Tensor: id=442939, shape=(1, 2856), dtype=float64, numpy=array([[1., 0., 0., ..., 0., 0., 0.]])>"
     ]
    }
   ],
   "source": [
    "am_input = tf.Variable(tf.random.normal((1, length)), trainable=True)\n",
    "iterations = 100\n",
    "learning_rate = tf.constant(1.0)\n",
    "for i in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(am_input)\n",
    "    am_gradient = tf.cast(tape.gradient(loss, am_input), tf.float64) \n",
    "    am_input.assign_add(learning_rate * tf.cast(am_gradient, tf.float32))\n",
    "    am_input = tf.Variable(tf.clip_by_value(am_input, 0, 1), trainable=True)\n",
    "    print(\"Loss: \", loss.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
