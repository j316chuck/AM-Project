{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design then implement \n",
    "# test everything you implement\n",
    "# test fast\n",
    "# code it :) \n",
    "# finish in 20 min \n",
    "\n",
    "\n",
    "# build out small test for 3 graphs. \n",
    "    # test for linear \n",
    "    # very interesting if we increase beta -> 10, then it converges around the vertex. \n",
    "    # But if we decrease beta -> 0.01, then it does not converge around the vertex but rather discrete minimums like -1, -1, 1, 1, 1,1 -1. \n",
    "    # same effect for f_quadratic and nn\n",
    "    # test for quadratic\n",
    "    # test for nn\n",
    "    \n",
    "# if you restrict your boundary, then your points on the simplex can only be within a certain range\n",
    "# however, if you have an infinite boundary, then your simplex can be found. (beta means nothing)\n",
    "# we want it to be within like [1, -1, -1, -1, -1] but its more like [-1, 1, -1, 1]\n",
    "# beta = 10 can get to like 1 num 1, but when beta = 0.01 get 5 num 1. Non-optimal because we want our large value to be maximized.\n",
    "# what if this is the global optimum*******************\n",
    "# if it is -> rethink research question (which is good)\n",
    "# nn -> output bounds (bounds are good, without bounds everything would be the same.) -> the global minimums are at 1, 1, -1, -1, multiple points with 1's\n",
    "# understand how the distribution of nn layers are. \n",
    "# understand how the \n",
    "    \n",
    "    \n",
    "# run experiment with random initialization 10 times, (take the minimum) add in change in beta, add in change in dimensions, \n",
    "# graph 3 different networks and plot\n",
    "\n",
    "\n",
    "# more rigorous graphs for random initializations\n",
    "# try different np-solvers\n",
    "# try different distributions initializations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_beta_x(x, beta = 1):\n",
    "    return np.exp(beta * x) / np.sum(np.exp(beta * x))\n",
    "\n",
    "\n",
    "def f_linear(inp, *args):\n",
    "    # negative parameter is when we want to convert max f(x) -> min -f(x). \n",
    "    # The x values are the same but the optimum f(x) is negated. \n",
    "    beta, dimension, negative = args[0]\n",
    "    x = softmax_beta_x(inp, beta)\n",
    "    np.testing.assert_almost_equal(np.sum(x), 1.0)\n",
    "    #x = inp\n",
    "    w, b = args[1]\n",
    "    assert(w.size == dimension and x.size == dimension)\n",
    "    if negative: \n",
    "        return -(w.dot(x) + b)\n",
    "    else: \n",
    "        return w.dot(x) + b\n",
    "            \n",
    "\n",
    "def f_linear_discrete(*args):\n",
    "    beta, dimension, negative = args[0]\n",
    "    w, b = args[1]\n",
    "    assert(w.size == dimension)\n",
    "    results = [w.dot(x) + b for x in np.eye(dimension)]\n",
    "    return results, np.argmax(results), np.max(results)\n",
    "\n",
    "\n",
    "def f_quadratic(inp, *args): \n",
    "    beta, dimension, negative = args[0]\n",
    "    x = softmax_beta_x(inp, beta)\n",
    "    Q, w, b = args[1]\n",
    "    assert(Q.shape == (dimension, dimension) \n",
    "           and x.size == dimension \n",
    "           and w.size == dimension)\n",
    "    if negative: \n",
    "        return -(x.T @ Q @ x + w.dot(x) + b)\n",
    "    else: \n",
    "        return x.T @ Q @ x + w.dot(x) + b\n",
    "\n",
    "    \n",
    "def f_quadratic_discrete(*args): \n",
    "    beta, dimension, negative = args[0]\n",
    "    Q, w, b = args[1]\n",
    "    assert(Q.shape == (dimension, dimension) \n",
    "           and w.size == dimension)\n",
    "    results = [x.T @ Q @ x + w.dot(x) + b for x in np.eye(dimension)]\n",
    "    return results, np.argmax(results), np.max(results)\n",
    "\n",
    "\n",
    "def f_neural_network(inp, *args): \n",
    "    beta, dimension, negative = args[0]\n",
    "    x = np.array([softmax_beta_x(inp, beta)])\n",
    "    for arg in args[1:]: \n",
    "        w, b, activation = arg[0], arg[1], arg[2]\n",
    "        x = x.dot(w) + b; \n",
    "        if activation == 'sigmoid':\n",
    "            x = 1/(1 + np.exp(-x))\n",
    "        elif activation == \"relu\": \n",
    "            x = np.maximum(x, 0)\n",
    "        else: \n",
    "            continue\n",
    "    if negative: \n",
    "        return -(x[0][0])\n",
    "    else: \n",
    "        return x[0][0]\n",
    "    \n",
    "\n",
    "def f_neural_network_discrete(*args): \n",
    "    beta, dimension, negative = args[0]\n",
    "    results = []\n",
    "    for elem in np.eye(dimension): \n",
    "        x = np.array(elem).T\n",
    "        for arg in args[1:]: \n",
    "            w, b, activation = arg[0], arg[1], arg[2]\n",
    "            x = x.dot(w) + b; \n",
    "            if activation == 'sigmoid':\n",
    "                x = 1/(1 + np.exp(-x))\n",
    "            elif activation == \"relu\": \n",
    "                x = np.maximum(x, 0)\n",
    "            else: \n",
    "                continue\n",
    "        results.append(x[0])\n",
    "    return results, np.argmax(results), np.max(results)\n",
    "\n",
    "\n",
    "def gen_linear_f(d, distribution):\n",
    "    if distribution == 'uniform':\n",
    "        return [[np.random.uniform(-1, 1, (d)), np.random.uniform(-1, 1)]]\n",
    "    elif distribution == 'normal':\n",
    "        return [[np.random.normal(0, 1, (d)), np.random.normal(0, 1)]]\n",
    "    else: \n",
    "        print(\"Distribution not supported\")\n",
    "        return -1 \n",
    "\n",
    "\n",
    "def gen_quadratic_f(d, distribution): \n",
    "    if distribution == \"uniform\": \n",
    "        return [[np.random.uniform(-1, 1, (d, d)), np.random.uniform(-1, 1, (d)), np.random.uniform(-1, 1)]]\n",
    "    elif distribution == \"normal\": \n",
    "        return [[np.random.normal(0, 1, (d, d)), np.random.normal(0, 1, (d)), np.random.normal(0, 1)]]\n",
    "    else: \n",
    "        print(\"Distribution not supported\")\n",
    "        return -1 \n",
    "    \n",
    "\n",
    "def gen_nn_f(d, distribution):\n",
    "    weights = []\n",
    "    for i in range(len(d) - 1): \n",
    "        w = np.random.normal(0, 1 / (d[i+1] ** 0.5), (d[i], d[i+1]))\n",
    "        b = np.random.normal(0, 1 / (d[i+1] ** 0.5), (d[i+1]))\n",
    "        if i == len(d) - 2: \n",
    "            weights.append([w, b, \"none\"]) \n",
    "        elif distribution == \"sigmoid\": \n",
    "            weights.append([w, b, \"sigmoid\"])\n",
    "        elif distribution == \"relu\":\n",
    "            weights.append([w, b, \"relu\"])\n",
    "        else: \n",
    "            print(\"Distribution not supported\")\n",
    "            return []\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete tests done\n",
    "dimensions = 3\n",
    "weights = gen_linear_f(dimensions, \"uniform\")\n",
    "#weights[0][0][0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights[0][0][0] = 1000\n",
    "verbose=False\n",
    "z_lst = []\n",
    "for i in range(1000): \n",
    "    x0 = np.random.uniform(-1, 1, (dimensions))\n",
    "    beta = 0.01\n",
    "    args = tuple([(beta, dimensions, True)] + weights)\n",
    "    bounds = [(-1, 1) for _ in range(dimensions)]\n",
    "    z = fmin_l_bfgs_b(f_linear, x0=x0, args=args, approx_grad=True,bounds=bounds, pgtol=1e-8)\n",
    "    args = list(args)\n",
    "    z_lst.append(z)\n",
    "    args[0] = [beta, dimensions, False]\n",
    "    args = tuple(args)\n",
    "    a, b = np.argwhere(weights[0][0] > 0).flatten(), np.argwhere(z[0] > 0).flatten()\n",
    "    if verbose: \n",
    "        print(\"Input: \", z[0])\n",
    "        #print(\"Grad < eps\", np.all(np.abs(z[2]['grad']) < 1e-3))\n",
    "        #print(\"Gradient\", z[2]['grad'])\n",
    "        print(\"Num of 1:\", sum(abs(1 - x) < 1e-5 for x in z[0]))\n",
    "        print(\"Softmax simplex: \", softmax_beta_x(z[0]))\n",
    "        print(\"Weights: \", weights[0][0])\n",
    "\n",
    "        print(\"Argmax weights: \", np.argmax(weights[0][0]), \" Argmax input: \", np.argmax(z[0]))\n",
    "        print(\"Value: \", -z[1])\n",
    "        print(\"f linear value: \", f_linear(z[0], *args))\n",
    "        print(\"Discrete value optimum point and value\", f_linear_discrete(*args)[1], f_linear_discrete(*args)[2])\n",
    "        print(a == b)\n",
    "        print(a)\n",
    "        print(b)\n",
    "        print(len(a))\n",
    "        print(len(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([-0.84979242,  0.56904581,  0.73410725]), -0.02424032394425324]]\n",
      "0.1335302491131125\n",
      "0.13077933621517815\n",
      "0.1268798877815863\n",
      "0.1268798877815863\n",
      "[-1.  1.  1.]\n",
      "0.1335302491131125\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(f_linear(np.array([-1., 1., 1.]), *args))\n",
    "print(f_linear(np.array([-1., -1., 1.]), *args)) \n",
    "print(f_linear(np.array([-1., -1., -1.]), *args))\n",
    "print(f_linear(np.array([1., 1., 1.]), *args))\n",
    "\n",
    "print(z_lst[0][0])\n",
    "print(-z_lst[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = np.array([z_lst[i][0] for i in range(1000)])\n",
    "np.isclose(tasks, tasks[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# test quadratic\\ndimensions = 100\\nx0 = np.random.uniform(-1, 1, (dimensions))\\nweights = gen_quadratic_f(dimensions, \"uniform\")\\nweights[0][0][0, 0] = 1000\\nbeta = 1.0\\nargs = tuple([(beta, dimensions, True)] + weights)\\nbounds = [(-1, 1) for _ in range(dimensions)]\\nz = fmin_l_bfgs_b(f_quadratic, x0=x0, args=args, approx_grad=True, bounds=bounds, pgtol=1e-8)\\nargs = list(args)\\nargs[0] = [beta, dimensions, False]\\nargs = tuple(args)\\nprint(z[0])\\nprint(-z[1])\\nprint(f_quadratic(z[0], *args))\\nf_quadratic_discrete(*args)[1], f_quadratic_discrete(*args)[2]\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# test quadratic\n",
    "dimensions = 100\n",
    "x0 = np.random.uniform(-1, 1, (dimensions))\n",
    "weights = gen_quadratic_f(dimensions, \"uniform\")\n",
    "weights[0][0][0, 0] = 1000\n",
    "beta = 1.0\n",
    "args = tuple([(beta, dimensions, True)] + weights)\n",
    "bounds = [(-1, 1) for _ in range(dimensions)]\n",
    "z = fmin_l_bfgs_b(f_quadratic, x0=x0, args=args, approx_grad=True, bounds=bounds, pgtol=1e-8)\n",
    "args = list(args)\n",
    "args[0] = [beta, dimensions, False]\n",
    "args = tuple(args)\n",
    "print(z[0])\n",
    "print(-z[1])\n",
    "print(f_quadratic(z[0], *args))\n",
    "f_quadratic_discrete(*args)[1], f_quadratic_discrete(*args)[2]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# test nn\\ndimensions = 10\\nx0 = np.random.uniform(-1, 1, (dimensions))\\nweights = gen_nn_f([dimensions, dimensions, 1], \"relu\")\\n#print(weights)\\n#weights[0][0][0,0] = 100\\n#weights[1][0][0] = 100\\n#print(weights)\\nbeta = 0.1\\nargs = tuple([(beta, dimensions, True)] + weights)\\nbounds = [(-1, 1) for _ in range(dimensions)]\\nz = fmin_l_bfgs_b(f_neural_network, x0=x0, args=args, approx_grad=True, bounds=bounds, pgtol=1e-8)\\nargs = list(args)\\nargs[0] = [beta, dimensions, False]\\nargs = tuple(args)\\nprint(z[0])\\nprint(-z[1])\\nprint(f_neural_network(z[0], *args))\\nf_neural_network_discrete(*args)[1], f_neural_network_discrete(*args)[2]\\n'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# test nn\n",
    "dimensions = 10\n",
    "x0 = np.random.uniform(-1, 1, (dimensions))\n",
    "weights = gen_nn_f([dimensions, dimensions, 1], \"relu\")\n",
    "#print(weights)\n",
    "#weights[0][0][0,0] = 100\n",
    "#weights[1][0][0] = 100\n",
    "#print(weights)\n",
    "beta = 0.1\n",
    "args = tuple([(beta, dimensions, True)] + weights)\n",
    "bounds = [(-1, 1) for _ in range(dimensions)]\n",
    "z = fmin_l_bfgs_b(f_neural_network, x0=x0, args=args, approx_grad=True, bounds=bounds, pgtol=1e-8)\n",
    "args = list(args)\n",
    "args[0] = [beta, dimensions, False]\n",
    "args = tuple(args)\n",
    "print(z[0])\n",
    "print(-z[1])\n",
    "print(f_neural_network(z[0], *args))\n",
    "f_neural_network_discrete(*args)[1], f_neural_network_discrete(*args)[2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = \"linear\"\n",
    "beta = 0.1\n",
    "trials\n",
    "\n",
    "if function == \"linear\": \n",
    "    w = gen_linear_f(d=dims, distribution=weight_init_args)\n",
    "    function = linear_f\n",
    "elif function == \"quadratic\": \n",
    "    w = gen_quadratic_f(d=dims, distribution=weight_init_args)\n",
    "    function = quadratic_f\n",
    "elif function == \"neural_network\": \n",
    "    w = gen_nn_f(d=layers, distribution=weight_init_args)\n",
    "    function = nn_f\n",
    "else: \n",
    "    print(\"Invalid function choice\")\n",
    "    return\n",
    "x0 = np.random.uniform(-1, 1, (dimensions))\n",
    "\n",
    "output = function(x, w)\n",
    "discrete_argmax_index = np.argmax(function(np.eye(dims), w))\n",
    "nearest_discrete_vertex = np.argmax(x, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(z, *params):\n",
    "    x, y = z\n",
    "    a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
    "    return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'anneal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-6d309f2128c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Initial guess.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m555\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Seeded to allow replication.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m res = anneal(f, x0, args=params, schedule='boltzmann',\n\u001b[1;32m      5\u001b[0m                       \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'anneal'"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2., 2.])     # Initial guess.\n",
    "np.random.seed(555)   # Seeded to allow replication.\n",
    "res = anneal(f, x0, args=params, schedule='boltzmann',\n",
    "                      full_output=True, maxiter=500, lower=-10,\n",
    "                          upper=10, dwell=250, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
